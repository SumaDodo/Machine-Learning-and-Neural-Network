{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sumas\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#setting variables\n",
    "ENV_NAME = 'CartPole-v0'\n",
    "\n",
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 34        \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 114\n",
      "Trainable params: 114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#building neural network\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sumas\\keras-rl\\rl\\memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   11/5000: episode: 1, duration: 1.126s, episode steps: 11, steps per second: 10, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.111 [-3.316, 2.196], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   21/5000: episode: 2, duration: 0.080s, episode steps: 10, steps per second: 126, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.167 [-1.905, 3.089], loss: 0.576204, mean_absolute_error: 0.973066, mean_q: 0.925027\n",
      "   31/5000: episode: 3, duration: 0.063s, episode steps: 10, steps per second: 158, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.157 [-3.116, 1.934], loss: 0.451600, mean_absolute_error: 0.805644, mean_q: 0.875803\n",
      "   39/5000: episode: 4, duration: 0.058s, episode steps: 8, steps per second: 139, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.138 [-2.563, 1.604], loss: 0.420353, mean_absolute_error: 0.796125, mean_q: 1.045240"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sumas\\keras-rl\\rl\\memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
      "C:\\Users\\sumas\\keras-rl\\rl\\memory.py:39: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   48/5000: episode: 5, duration: 0.062s, episode steps: 9, steps per second: 146, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.154 [-1.753, 2.794], loss: 0.414353, mean_absolute_error: 0.788617, mean_q: 1.187130\n",
      "   57/5000: episode: 6, duration: 0.053s, episode steps: 9, steps per second: 170, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.134 [-2.454, 1.608], loss: 0.355994, mean_absolute_error: 0.673336, mean_q: 1.122542\n",
      "   66/5000: episode: 7, duration: 0.062s, episode steps: 9, steps per second: 146, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.138 [-2.817, 1.757], loss: 0.434367, mean_absolute_error: 0.714431, mean_q: 1.301025\n",
      "   75/5000: episode: 8, duration: 0.065s, episode steps: 9, steps per second: 138, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.118 [-2.741, 1.805], loss: 0.400932, mean_absolute_error: 0.669487, mean_q: 1.315506\n",
      "   85/5000: episode: 9, duration: 0.056s, episode steps: 10, steps per second: 177, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.142 [-2.996, 1.937], loss: 0.407313, mean_absolute_error: 0.651223, mean_q: 1.426309\n",
      "   95/5000: episode: 10, duration: 0.072s, episode steps: 10, steps per second: 139, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.129 [-2.540, 1.540], loss: 0.494111, mean_absolute_error: 0.671954, mean_q: 1.547066\n",
      "  104/5000: episode: 11, duration: 0.070s, episode steps: 9, steps per second: 129, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.136 [-2.795, 1.802], loss: 0.423267, mean_absolute_error: 0.608487, mean_q: 1.538342\n",
      "  113/5000: episode: 12, duration: 0.066s, episode steps: 9, steps per second: 136, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.144 [-2.881, 1.804], loss: 0.470390, mean_absolute_error: 0.635970, mean_q: 1.746727\n",
      "  122/5000: episode: 13, duration: 0.055s, episode steps: 9, steps per second: 165, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.142 [-2.812, 1.778], loss: 0.512096, mean_absolute_error: 0.611804, mean_q: 1.791992\n",
      "  133/5000: episode: 14, duration: 0.073s, episode steps: 11, steps per second: 150, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.146 [-2.816, 1.755], loss: 0.484410, mean_absolute_error: 0.569689, mean_q: 1.826976\n",
      "  144/5000: episode: 15, duration: 0.081s, episode steps: 11, steps per second: 136, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.114 [-2.738, 1.768], loss: 0.641157, mean_absolute_error: 0.632838, mean_q: 1.986633\n",
      "  153/5000: episode: 16, duration: 0.068s, episode steps: 9, steps per second: 133, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.167 [-2.873, 1.780], loss: 0.549037, mean_absolute_error: 0.561321, mean_q: 1.960919\n",
      "  162/5000: episode: 17, duration: 0.067s, episode steps: 9, steps per second: 135, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.152 [-2.835, 1.751], loss: 0.560819, mean_absolute_error: 0.573357, mean_q: 2.002764\n",
      "  171/5000: episode: 18, duration: 0.072s, episode steps: 9, steps per second: 125, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.121 [-2.779, 1.785], loss: 0.623382, mean_absolute_error: 0.623543, mean_q: 2.011349\n",
      "  181/5000: episode: 19, duration: 0.049s, episode steps: 10, steps per second: 203, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.141 [-3.091, 1.961], loss: 0.524388, mean_absolute_error: 0.604710, mean_q: 2.036528\n",
      "  193/5000: episode: 20, duration: 0.091s, episode steps: 12, steps per second: 132, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.087 [-2.995, 1.974], loss: 0.608141, mean_absolute_error: 0.656432, mean_q: 2.231351\n",
      "  202/5000: episode: 21, duration: 0.062s, episode steps: 9, steps per second: 144, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.133 [-2.743, 1.748], loss: 0.699877, mean_absolute_error: 0.730806, mean_q: 2.275017\n",
      "  212/5000: episode: 22, duration: 0.062s, episode steps: 10, steps per second: 161, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.116 [-2.982, 1.989], loss: 0.556454, mean_absolute_error: 0.710550, mean_q: 2.300653\n",
      "  222/5000: episode: 23, duration: 0.057s, episode steps: 10, steps per second: 174, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.136 [-3.099, 1.970], loss: 0.591462, mean_absolute_error: 0.760755, mean_q: 2.368768\n",
      "  231/5000: episode: 24, duration: 0.050s, episode steps: 9, steps per second: 179, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.173 [-2.861, 1.748], loss: 0.752219, mean_absolute_error: 0.817057, mean_q: 2.460959\n",
      "  241/5000: episode: 25, duration: 0.057s, episode steps: 10, steps per second: 175, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.130 [-2.984, 1.969], loss: 0.558114, mean_absolute_error: 0.787795, mean_q: 2.398457\n",
      "  250/5000: episode: 26, duration: 0.051s, episode steps: 9, steps per second: 177, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.175 [-2.889, 1.752], loss: 0.761707, mean_absolute_error: 0.871243, mean_q: 2.571345\n",
      "  260/5000: episode: 27, duration: 0.050s, episode steps: 10, steps per second: 200, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.156 [-3.026, 1.906], loss: 0.641712, mean_absolute_error: 0.856012, mean_q: 2.564124\n",
      "  271/5000: episode: 28, duration: 0.055s, episode steps: 11, steps per second: 202, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.103 [-2.293, 1.420], loss: 0.774229, mean_absolute_error: 0.929494, mean_q: 2.765554\n",
      "  280/5000: episode: 29, duration: 0.059s, episode steps: 9, steps per second: 154, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.131 [-2.760, 1.804], loss: 0.702597, mean_absolute_error: 0.932416, mean_q: 2.737206\n",
      "  291/5000: episode: 30, duration: 0.060s, episode steps: 11, steps per second: 185, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.139 [-2.870, 1.731], loss: 0.668636, mean_absolute_error: 0.941896, mean_q: 2.800610\n",
      "  301/5000: episode: 31, duration: 0.066s, episode steps: 10, steps per second: 152, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.153 [-3.016, 1.909], loss: 0.595485, mean_absolute_error: 0.942059, mean_q: 2.884437\n",
      "  312/5000: episode: 32, duration: 0.074s, episode steps: 11, steps per second: 148, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.111 [-2.806, 1.784], loss: 0.701999, mean_absolute_error: 0.983877, mean_q: 3.042351\n",
      "  322/5000: episode: 33, duration: 0.058s, episode steps: 10, steps per second: 173, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.129 [-2.491, 1.616], loss: 0.734032, mean_absolute_error: 1.037541, mean_q: 3.110374\n",
      "  332/5000: episode: 34, duration: 0.054s, episode steps: 10, steps per second: 184, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.130 [-2.972, 1.930], loss: 0.679060, mean_absolute_error: 1.061825, mean_q: 3.042269\n",
      "  341/5000: episode: 35, duration: 0.054s, episode steps: 9, steps per second: 167, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.159 [-2.825, 1.740], loss: 0.699130, mean_absolute_error: 1.125580, mean_q: 3.161163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  351/5000: episode: 36, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.131 [-2.596, 1.586], loss: 0.813166, mean_absolute_error: 1.209408, mean_q: 3.258272\n",
      "  361/5000: episode: 37, duration: 0.051s, episode steps: 10, steps per second: 196, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.131 [-3.004, 1.973], loss: 0.766104, mean_absolute_error: 1.250832, mean_q: 3.253578\n",
      "  369/5000: episode: 38, duration: 0.053s, episode steps: 8, steps per second: 150, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.138 [-2.490, 1.562], loss: 0.643470, mean_absolute_error: 1.245947, mean_q: 3.205327\n",
      "  381/5000: episode: 39, duration: 0.062s, episode steps: 12, steps per second: 194, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.116 [-3.032, 1.941], loss: 0.736281, mean_absolute_error: 1.285205, mean_q: 3.292563\n",
      "  393/5000: episode: 40, duration: 0.088s, episode steps: 12, steps per second: 136, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.094 [-2.994, 1.956], loss: 0.726526, mean_absolute_error: 1.365686, mean_q: 3.370937\n",
      "  405/5000: episode: 41, duration: 0.080s, episode steps: 12, steps per second: 149, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.126 [-2.541, 1.520], loss: 0.764244, mean_absolute_error: 1.413217, mean_q: 3.415441\n",
      "  415/5000: episode: 42, duration: 0.074s, episode steps: 10, steps per second: 135, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.099 [-2.945, 1.990], loss: 0.659833, mean_absolute_error: 1.419725, mean_q: 3.489234\n",
      "  426/5000: episode: 43, duration: 0.058s, episode steps: 11, steps per second: 188, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.116 [-2.859, 1.802], loss: 0.594772, mean_absolute_error: 1.429167, mean_q: 3.546776\n",
      "  436/5000: episode: 44, duration: 0.041s, episode steps: 10, steps per second: 243, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.127 [-3.055, 1.995], loss: 0.557389, mean_absolute_error: 1.443390, mean_q: 3.667552\n",
      "  446/5000: episode: 45, duration: 0.061s, episode steps: 10, steps per second: 163, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.106 [-2.992, 1.989], loss: 0.615588, mean_absolute_error: 1.501139, mean_q: 3.772928\n",
      "  456/5000: episode: 46, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.110 [-2.521, 1.606], loss: 0.745618, mean_absolute_error: 1.564925, mean_q: 3.780202\n",
      "  468/5000: episode: 47, duration: 0.058s, episode steps: 12, steps per second: 208, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.126 [-2.495, 1.533], loss: 0.649626, mean_absolute_error: 1.626056, mean_q: 3.799782\n",
      "  478/5000: episode: 48, duration: 0.048s, episode steps: 10, steps per second: 210, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.151 [-3.103, 1.949], loss: 0.608956, mean_absolute_error: 1.631487, mean_q: 3.839294\n",
      "  487/5000: episode: 49, duration: 0.041s, episode steps: 9, steps per second: 219, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.170 [-2.857, 1.741], loss: 0.916538, mean_absolute_error: 1.788642, mean_q: 3.953340\n",
      "  496/5000: episode: 50, duration: 0.064s, episode steps: 9, steps per second: 140, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.143 [-2.795, 1.795], loss: 0.622620, mean_absolute_error: 1.759346, mean_q: 3.869637\n",
      "  504/5000: episode: 51, duration: 0.041s, episode steps: 8, steps per second: 196, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.150 [-2.537, 1.534], loss: 0.750286, mean_absolute_error: 1.846044, mean_q: 3.914787\n",
      "  513/5000: episode: 52, duration: 0.049s, episode steps: 9, steps per second: 183, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.134 [-1.857, 1.133], loss: 0.797161, mean_absolute_error: 1.902289, mean_q: 3.922826\n",
      "  523/5000: episode: 53, duration: 0.052s, episode steps: 10, steps per second: 194, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.145 [-2.553, 1.563], loss: 0.604032, mean_absolute_error: 1.892839, mean_q: 3.978462\n",
      "  533/5000: episode: 54, duration: 0.067s, episode steps: 10, steps per second: 148, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.149 [-3.083, 1.975], loss: 0.710021, mean_absolute_error: 1.937942, mean_q: 4.100101\n",
      "  542/5000: episode: 55, duration: 0.062s, episode steps: 9, steps per second: 144, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.170 [-2.842, 1.733], loss: 0.661698, mean_absolute_error: 2.006009, mean_q: 4.143131\n",
      "  552/5000: episode: 56, duration: 0.063s, episode steps: 10, steps per second: 159, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.146 [-2.746, 1.746], loss: 0.656116, mean_absolute_error: 2.043751, mean_q: 4.172105\n",
      "  564/5000: episode: 57, duration: 0.072s, episode steps: 12, steps per second: 167, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.121 [-2.591, 1.599], loss: 0.607197, mean_absolute_error: 2.067423, mean_q: 4.235333\n",
      "  574/5000: episode: 58, duration: 0.063s, episode steps: 10, steps per second: 160, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.130 [-2.628, 1.719], loss: 0.526927, mean_absolute_error: 2.031267, mean_q: 4.359007\n",
      "  584/5000: episode: 59, duration: 0.055s, episode steps: 10, steps per second: 183, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.127 [-2.205, 1.330], loss: 0.613911, mean_absolute_error: 2.112149, mean_q: 4.433001\n",
      "  594/5000: episode: 60, duration: 0.067s, episode steps: 10, steps per second: 148, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.153 [-2.452, 1.552], loss: 0.620151, mean_absolute_error: 2.183963, mean_q: 4.467638\n",
      "  603/5000: episode: 61, duration: 0.067s, episode steps: 9, steps per second: 134, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.154 [-2.471, 1.570], loss: 0.630289, mean_absolute_error: 2.199931, mean_q: 4.518671\n",
      "  613/5000: episode: 62, duration: 0.094s, episode steps: 10, steps per second: 106, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.117 [-2.275, 1.413], loss: 0.556729, mean_absolute_error: 2.223186, mean_q: 4.607493\n",
      "  622/5000: episode: 63, duration: 0.052s, episode steps: 9, steps per second: 172, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.143 [-2.482, 1.607], loss: 0.538363, mean_absolute_error: 2.178561, mean_q: 4.734483\n",
      "  630/5000: episode: 64, duration: 0.063s, episode steps: 8, steps per second: 127, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.875 [0.000, 1.000], mean observation: -0.150 [-2.183, 1.356], loss: 0.664307, mean_absolute_error: 2.253297, mean_q: 4.656185\n",
      "  641/5000: episode: 65, duration: 0.068s, episode steps: 11, steps per second: 162, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.112 [-1.639, 1.000], loss: 0.606036, mean_absolute_error: 2.239288, mean_q: 4.614649\n",
      "  649/5000: episode: 66, duration: 0.058s, episode steps: 8, steps per second: 138, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.875 [0.000, 1.000], mean observation: -0.142 [-2.185, 1.366], loss: 0.536952, mean_absolute_error: 2.215254, mean_q: 4.732314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  659/5000: episode: 67, duration: 0.059s, episode steps: 10, steps per second: 169, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.121 [-2.377, 1.584], loss: 0.572910, mean_absolute_error: 2.239264, mean_q: 4.702991\n",
      "  668/5000: episode: 68, duration: 0.049s, episode steps: 9, steps per second: 185, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.147 [-2.257, 1.386], loss: 0.586389, mean_absolute_error: 2.246960, mean_q: 4.717199\n",
      "  677/5000: episode: 69, duration: 0.050s, episode steps: 9, steps per second: 178, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.124 [-2.235, 1.387], loss: 0.542446, mean_absolute_error: 2.234447, mean_q: 4.752459\n",
      "  688/5000: episode: 70, duration: 0.089s, episode steps: 11, steps per second: 123, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.127 [-2.346, 1.517], loss: 0.434014, mean_absolute_error: 2.151147, mean_q: 4.861554\n",
      "  699/5000: episode: 71, duration: 0.063s, episode steps: 11, steps per second: 174, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.116 [-2.386, 1.593], loss: 0.438058, mean_absolute_error: 2.165871, mean_q: 4.905065\n",
      "  712/5000: episode: 72, duration: 0.087s, episode steps: 13, steps per second: 150, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.119 [-2.463, 1.521], loss: 0.404742, mean_absolute_error: 2.126653, mean_q: 4.908219\n",
      "  723/5000: episode: 73, duration: 0.059s, episode steps: 11, steps per second: 187, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.110 [-2.407, 1.598], loss: 0.439777, mean_absolute_error: 2.195073, mean_q: 5.034952\n",
      "  733/5000: episode: 74, duration: 0.073s, episode steps: 10, steps per second: 137, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.130 [-2.519, 1.587], loss: 0.402041, mean_absolute_error: 2.179530, mean_q: 5.073515\n",
      "  745/5000: episode: 75, duration: 0.084s, episode steps: 12, steps per second: 144, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.085 [-2.394, 1.606], loss: 0.384614, mean_absolute_error: 2.197087, mean_q: 5.134107\n",
      "  755/5000: episode: 76, duration: 0.073s, episode steps: 10, steps per second: 138, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.136 [-2.285, 1.397], loss: 0.454596, mean_absolute_error: 2.266714, mean_q: 5.062509\n",
      "  766/5000: episode: 77, duration: 0.060s, episode steps: 11, steps per second: 183, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.113 [-2.353, 1.603], loss: 0.458109, mean_absolute_error: 2.311009, mean_q: 5.107172\n",
      "  777/5000: episode: 78, duration: 0.063s, episode steps: 11, steps per second: 175, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.111 [-2.233, 1.407], loss: 0.463004, mean_absolute_error: 2.368289, mean_q: 5.161425\n",
      "  788/5000: episode: 79, duration: 0.068s, episode steps: 11, steps per second: 162, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.104 [-2.209, 1.414], loss: 0.402777, mean_absolute_error: 2.384706, mean_q: 5.352147\n",
      "  799/5000: episode: 80, duration: 0.061s, episode steps: 11, steps per second: 182, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.109 [-2.134, 1.383], loss: 0.494993, mean_absolute_error: 2.438600, mean_q: 5.205585\n",
      "  809/5000: episode: 81, duration: 0.064s, episode steps: 10, steps per second: 156, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.119 [-2.210, 1.414], loss: 0.438435, mean_absolute_error: 2.443993, mean_q: 5.348994\n",
      "  818/5000: episode: 82, duration: 0.062s, episode steps: 9, steps per second: 145, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.132 [-2.122, 1.346], loss: 0.545407, mean_absolute_error: 2.492436, mean_q: 5.270670\n",
      "  829/5000: episode: 83, duration: 0.065s, episode steps: 11, steps per second: 169, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.126 [-1.971, 1.222], loss: 0.449996, mean_absolute_error: 2.495043, mean_q: 5.390633\n",
      "  839/5000: episode: 84, duration: 0.054s, episode steps: 10, steps per second: 184, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.122 [-2.160, 1.359], loss: 0.503723, mean_absolute_error: 2.529907, mean_q: 5.505064\n",
      "  849/5000: episode: 85, duration: 0.064s, episode steps: 10, steps per second: 156, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.131 [-2.126, 1.378], loss: 0.408387, mean_absolute_error: 2.465647, mean_q: 5.316220\n",
      "  863/5000: episode: 86, duration: 0.097s, episode steps: 14, steps per second: 144, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.098 [-2.102, 1.230], loss: 0.427906, mean_absolute_error: 2.530521, mean_q: 5.482182\n",
      "  872/5000: episode: 87, duration: 0.066s, episode steps: 9, steps per second: 137, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.150 [-2.183, 1.336], loss: 0.577124, mean_absolute_error: 2.577938, mean_q: 5.419493\n",
      "  883/5000: episode: 88, duration: 0.068s, episode steps: 11, steps per second: 163, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.124 [-2.186, 1.346], loss: 0.424634, mean_absolute_error: 2.547188, mean_q: 5.596290\n",
      "  892/5000: episode: 89, duration: 0.059s, episode steps: 9, steps per second: 152, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.142 [-2.196, 1.411], loss: 0.462225, mean_absolute_error: 2.527886, mean_q: 5.452016\n",
      "  900/5000: episode: 90, duration: 0.052s, episode steps: 8, steps per second: 153, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.875 [0.000, 1.000], mean observation: -0.142 [-2.256, 1.421], loss: 0.398978, mean_absolute_error: 2.554093, mean_q: 5.593781\n",
      "  909/5000: episode: 91, duration: 0.058s, episode steps: 9, steps per second: 155, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.136 [-2.160, 1.347], loss: 0.433640, mean_absolute_error: 2.593748, mean_q: 5.655287\n",
      "  918/5000: episode: 92, duration: 0.059s, episode steps: 9, steps per second: 153, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.155 [-2.296, 1.371], loss: 0.462672, mean_absolute_error: 2.587770, mean_q: 5.503193\n",
      "  927/5000: episode: 93, duration: 0.044s, episode steps: 9, steps per second: 204, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.147 [-2.460, 1.543], loss: 0.494717, mean_absolute_error: 2.674114, mean_q: 5.649672\n",
      "  942/5000: episode: 94, duration: 0.096s, episode steps: 15, steps per second: 156, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.048 [-2.198, 1.593], loss: 0.337965, mean_absolute_error: 2.635859, mean_q: 5.787491\n",
      "  952/5000: episode: 95, duration: 0.055s, episode steps: 10, steps per second: 181, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.131 [-2.224, 1.418], loss: 0.384940, mean_absolute_error: 2.713523, mean_q: 5.865329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  960/5000: episode: 96, duration: 0.049s, episode steps: 8, steps per second: 163, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.875 [0.000, 1.000], mean observation: -0.142 [-2.168, 1.356], loss: 0.382800, mean_absolute_error: 2.712770, mean_q: 5.726696\n",
      "  975/5000: episode: 97, duration: 0.090s, episode steps: 15, steps per second: 166, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.061 [-2.222, 1.585], loss: 0.435596, mean_absolute_error: 2.707220, mean_q: 5.732056\n",
      "  986/5000: episode: 98, duration: 0.060s, episode steps: 11, steps per second: 185, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.117 [-2.105, 1.359], loss: 0.672781, mean_absolute_error: 2.798894, mean_q: 5.789237\n",
      "  995/5000: episode: 99, duration: 0.065s, episode steps: 9, steps per second: 139, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.177 [-2.541, 1.526], loss: 0.633413, mean_absolute_error: 2.746749, mean_q: 5.693252\n",
      " 1004/5000: episode: 100, duration: 0.052s, episode steps: 9, steps per second: 175, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.151 [-2.190, 1.378], loss: 0.433715, mean_absolute_error: 2.800522, mean_q: 5.926198\n",
      " 1013/5000: episode: 101, duration: 0.050s, episode steps: 9, steps per second: 179, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.152 [-2.228, 1.361], loss: 0.466242, mean_absolute_error: 2.765310, mean_q: 5.874102\n",
      " 1021/5000: episode: 102, duration: 0.041s, episode steps: 8, steps per second: 195, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.875 [0.000, 1.000], mean observation: -0.128 [-2.201, 1.379], loss: 0.479403, mean_absolute_error: 2.801589, mean_q: 5.954936\n",
      " 1033/5000: episode: 103, duration: 0.067s, episode steps: 12, steps per second: 179, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.111 [-2.137, 1.371], loss: 0.380853, mean_absolute_error: 2.817983, mean_q: 5.972310\n",
      " 1042/5000: episode: 104, duration: 0.047s, episode steps: 9, steps per second: 192, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.149 [-2.203, 1.354], loss: 0.500083, mean_absolute_error: 2.827260, mean_q: 5.967280\n",
      " 1052/5000: episode: 105, duration: 0.047s, episode steps: 10, steps per second: 212, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.138 [-2.025, 1.173], loss: 0.504394, mean_absolute_error: 2.871294, mean_q: 6.048010\n",
      " 1062/5000: episode: 106, duration: 0.074s, episode steps: 10, steps per second: 135, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.100 [-2.170, 1.386], loss: 0.495831, mean_absolute_error: 2.886021, mean_q: 6.096262\n",
      " 1073/5000: episode: 107, duration: 0.065s, episode steps: 11, steps per second: 169, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.083 [-2.121, 1.412], loss: 0.367478, mean_absolute_error: 2.862133, mean_q: 6.066844\n",
      " 1084/5000: episode: 108, duration: 0.076s, episode steps: 11, steps per second: 145, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.104 [-2.277, 1.594], loss: 0.415133, mean_absolute_error: 2.909822, mean_q: 6.240441\n",
      " 1094/5000: episode: 109, duration: 0.060s, episode steps: 10, steps per second: 165, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.144 [-2.185, 1.372], loss: 0.442398, mean_absolute_error: 2.909281, mean_q: 6.166405\n",
      " 1104/5000: episode: 110, duration: 0.061s, episode steps: 10, steps per second: 164, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.141 [-2.105, 1.357], loss: 0.377828, mean_absolute_error: 2.953265, mean_q: 6.174058\n",
      " 1113/5000: episode: 111, duration: 0.050s, episode steps: 9, steps per second: 180, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.131 [-1.939, 1.196], loss: 0.478274, mean_absolute_error: 2.964855, mean_q: 6.194620\n",
      " 1123/5000: episode: 112, duration: 0.065s, episode steps: 10, steps per second: 155, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.106 [-2.100, 1.394], loss: 0.297448, mean_absolute_error: 2.871548, mean_q: 6.015170\n",
      " 1134/5000: episode: 113, duration: 0.071s, episode steps: 11, steps per second: 155, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.133 [-2.078, 1.333], loss: 0.297345, mean_absolute_error: 2.988487, mean_q: 6.308685\n",
      " 1145/5000: episode: 114, duration: 0.067s, episode steps: 11, steps per second: 165, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.133 [-2.059, 1.331], loss: 0.386806, mean_absolute_error: 3.051973, mean_q: 6.238548\n",
      " 1156/5000: episode: 115, duration: 0.063s, episode steps: 11, steps per second: 176, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.110 [-2.021, 1.337], loss: 0.349801, mean_absolute_error: 3.093363, mean_q: 6.313393\n",
      " 1165/5000: episode: 116, duration: 0.053s, episode steps: 9, steps per second: 169, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.139 [-1.913, 1.184], loss: 0.263519, mean_absolute_error: 3.021317, mean_q: 6.186219\n",
      " 1174/5000: episode: 117, duration: 0.060s, episode steps: 9, steps per second: 150, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.140 [-1.848, 1.149], loss: 0.325025, mean_absolute_error: 3.140173, mean_q: 6.359249\n",
      " 1182/5000: episode: 118, duration: 0.040s, episode steps: 8, steps per second: 200, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.142 [-1.902, 1.185], loss: 0.211587, mean_absolute_error: 3.218014, mean_q: 6.597721\n",
      " 1191/5000: episode: 119, duration: 0.053s, episode steps: 9, steps per second: 170, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.128 [-1.649, 1.026], loss: 0.267041, mean_absolute_error: 3.023108, mean_q: 6.053555\n",
      " 1201/5000: episode: 120, duration: 0.055s, episode steps: 10, steps per second: 183, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.152 [-1.851, 1.138], loss: 0.379187, mean_absolute_error: 3.126053, mean_q: 6.127983\n",
      " 1213/5000: episode: 121, duration: 0.079s, episode steps: 12, steps per second: 151, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.093 [-1.733, 1.157], loss: 0.482479, mean_absolute_error: 3.159595, mean_q: 6.159742\n",
      " 1224/5000: episode: 122, duration: 0.070s, episode steps: 11, steps per second: 156, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.125 [-1.623, 0.943], loss: 0.358225, mean_absolute_error: 3.232819, mean_q: 6.342499\n",
      " 1236/5000: episode: 123, duration: 0.073s, episode steps: 12, steps per second: 165, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.105 [-1.888, 1.178], loss: 0.512217, mean_absolute_error: 3.351221, mean_q: 6.582577\n",
      " 1247/5000: episode: 124, duration: 0.055s, episode steps: 11, steps per second: 202, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.128 [-2.298, 1.370], loss: 0.787220, mean_absolute_error: 3.319447, mean_q: 6.447929\n",
      " 1256/5000: episode: 125, duration: 0.052s, episode steps: 9, steps per second: 171, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.140 [-1.665, 1.010], loss: 0.553782, mean_absolute_error: 3.451728, mean_q: 6.810604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1265/5000: episode: 126, duration: 0.053s, episode steps: 9, steps per second: 169, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.120 [-1.648, 1.016], loss: 0.318958, mean_absolute_error: 3.334663, mean_q: 6.592852\n",
      " 1277/5000: episode: 127, duration: 0.084s, episode steps: 12, steps per second: 143, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.092 [-1.730, 1.216], loss: 0.366692, mean_absolute_error: 3.207676, mean_q: 6.265222\n",
      " 1289/5000: episode: 128, duration: 0.086s, episode steps: 12, steps per second: 140, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.113 [-1.760, 1.153], loss: 0.404219, mean_absolute_error: 3.278796, mean_q: 6.342018\n",
      " 1301/5000: episode: 129, duration: 0.083s, episode steps: 12, steps per second: 145, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.107 [-1.769, 1.150], loss: 0.958778, mean_absolute_error: 3.431832, mean_q: 6.617478\n",
      " 1314/5000: episode: 130, duration: 0.093s, episode steps: 13, steps per second: 140, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.111 [-1.618, 0.963], loss: 0.517281, mean_absolute_error: 3.319173, mean_q: 6.450045\n",
      " 1326/5000: episode: 131, duration: 0.086s, episode steps: 12, steps per second: 140, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.081 [-1.526, 0.993], loss: 0.323028, mean_absolute_error: 3.368518, mean_q: 6.578856\n",
      " 1339/5000: episode: 132, duration: 0.075s, episode steps: 13, steps per second: 173, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.093 [-1.482, 1.007], loss: 0.358594, mean_absolute_error: 3.391354, mean_q: 6.549260\n",
      " 1351/5000: episode: 133, duration: 0.062s, episode steps: 12, steps per second: 193, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.095 [-1.810, 1.195], loss: 0.311078, mean_absolute_error: 3.363056, mean_q: 6.506176\n",
      " 1362/5000: episode: 134, duration: 0.044s, episode steps: 11, steps per second: 253, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.122 [-1.311, 0.770], loss: 0.348787, mean_absolute_error: 3.502605, mean_q: 6.790776\n",
      " 1375/5000: episode: 135, duration: 0.077s, episode steps: 13, steps per second: 168, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.110 [-1.493, 0.936], loss: 0.378946, mean_absolute_error: 3.353316, mean_q: 6.421659\n",
      " 1387/5000: episode: 136, duration: 0.080s, episode steps: 12, steps per second: 150, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.119 [-1.341, 0.790], loss: 0.732943, mean_absolute_error: 3.565527, mean_q: 6.838661\n",
      " 1402/5000: episode: 137, duration: 0.102s, episode steps: 15, steps per second: 146, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.086 [-1.273, 0.807], loss: 0.420399, mean_absolute_error: 3.488720, mean_q: 6.686500\n",
      " 1416/5000: episode: 138, duration: 0.080s, episode steps: 14, steps per second: 175, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.099 [-1.206, 0.738], loss: 0.560042, mean_absolute_error: 3.478392, mean_q: 6.587186\n",
      " 1427/5000: episode: 139, duration: 0.068s, episode steps: 11, steps per second: 163, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.123 [-1.254, 0.787], loss: 0.595096, mean_absolute_error: 3.555446, mean_q: 6.741463\n",
      " 1441/5000: episode: 140, duration: 0.081s, episode steps: 14, steps per second: 173, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.103 [-1.245, 0.763], loss: 0.313356, mean_absolute_error: 3.601017, mean_q: 6.935512\n",
      " 1452/5000: episode: 141, duration: 0.062s, episode steps: 11, steps per second: 178, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.120 [-1.162, 0.584], loss: 0.410016, mean_absolute_error: 3.544231, mean_q: 6.728726\n",
      " 1476/5000: episode: 142, duration: 0.139s, episode steps: 24, steps per second: 172, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.030 [-0.849, 0.630], loss: 0.463015, mean_absolute_error: 3.624820, mean_q: 6.790802\n",
      " 1493/5000: episode: 143, duration: 0.112s, episode steps: 17, steps per second: 152, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.094 [-1.136, 0.758], loss: 0.406105, mean_absolute_error: 3.640157, mean_q: 6.871350\n",
      " 1519/5000: episode: 144, duration: 0.136s, episode steps: 26, steps per second: 191, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.067 [-0.832, 0.604], loss: 0.335711, mean_absolute_error: 3.740787, mean_q: 7.073125\n",
      " 1563/5000: episode: 145, duration: 0.249s, episode steps: 44, steps per second: 177, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.022 [-0.788, 0.853], loss: 0.543476, mean_absolute_error: 3.811765, mean_q: 7.186062\n",
      " 1578/5000: episode: 146, duration: 0.084s, episode steps: 15, steps per second: 179, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.076 [-0.817, 1.218], loss: 0.760157, mean_absolute_error: 3.890581, mean_q: 7.364023\n",
      " 1589/5000: episode: 147, duration: 0.066s, episode steps: 11, steps per second: 166, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.128 [-0.977, 1.573], loss: 0.678225, mean_absolute_error: 3.983495, mean_q: 7.496473\n",
      " 1605/5000: episode: 148, duration: 0.090s, episode steps: 16, steps per second: 177, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.093 [-0.781, 1.258], loss: 0.527526, mean_absolute_error: 3.974944, mean_q: 7.564565\n",
      " 1622/5000: episode: 149, duration: 0.091s, episode steps: 17, steps per second: 188, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.063 [-0.792, 1.189], loss: 0.365690, mean_absolute_error: 3.912109, mean_q: 7.494579\n",
      " 1633/5000: episode: 150, duration: 0.056s, episode steps: 11, steps per second: 198, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.105 [-0.979, 1.568], loss: 0.590255, mean_absolute_error: 4.018296, mean_q: 7.651060\n",
      " 1643/5000: episode: 151, duration: 0.059s, episode steps: 10, steps per second: 171, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.098 [-1.027, 1.583], loss: 1.201769, mean_absolute_error: 4.247031, mean_q: 7.984415\n",
      " 1653/5000: episode: 152, duration: 0.081s, episode steps: 10, steps per second: 123, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.149 [-1.561, 2.570], loss: 0.537409, mean_absolute_error: 4.184528, mean_q: 7.987740\n",
      " 1662/5000: episode: 153, duration: 0.069s, episode steps: 9, steps per second: 130, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.127 [-1.588, 2.414], loss: 0.562021, mean_absolute_error: 4.041731, mean_q: 7.678299\n",
      " 1675/5000: episode: 154, duration: 0.066s, episode steps: 13, steps per second: 196, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.117 [-1.365, 2.324], loss: 0.752959, mean_absolute_error: 4.177849, mean_q: 7.958128\n",
      " 1686/5000: episode: 155, duration: 0.060s, episode steps: 11, steps per second: 183, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.124 [-1.391, 2.239], loss: 1.464049, mean_absolute_error: 4.396830, mean_q: 8.317468\n",
      " 1694/5000: episode: 156, duration: 0.062s, episode steps: 8, steps per second: 129, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.146 [-1.590, 2.609], loss: 1.671750, mean_absolute_error: 4.341699, mean_q: 8.081694\n",
      " 1704/5000: episode: 157, duration: 0.067s, episode steps: 10, steps per second: 150, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.148 [-1.947, 3.086], loss: 0.681027, mean_absolute_error: 4.304921, mean_q: 8.120030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1712/5000: episode: 158, duration: 0.067s, episode steps: 8, steps per second: 120, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.153 [-1.538, 2.552], loss: 0.819311, mean_absolute_error: 4.340707, mean_q: 8.221566\n",
      " 1727/5000: episode: 159, duration: 0.092s, episode steps: 15, steps per second: 162, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.099 [-1.706, 2.719], loss: 1.571523, mean_absolute_error: 4.511755, mean_q: 8.537303\n",
      " 1735/5000: episode: 160, duration: 0.046s, episode steps: 8, steps per second: 172, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.166 [-1.578, 2.590], loss: 1.271868, mean_absolute_error: 4.577639, mean_q: 8.604052\n",
      " 1743/5000: episode: 161, duration: 0.056s, episode steps: 8, steps per second: 142, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.170 [-1.547, 2.570], loss: 0.689303, mean_absolute_error: 4.449291, mean_q: 8.450489\n",
      " 1754/5000: episode: 162, duration: 0.071s, episode steps: 11, steps per second: 155, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.117 [-1.604, 2.452], loss: 1.562108, mean_absolute_error: 4.686033, mean_q: 8.846641\n",
      " 1762/5000: episode: 163, duration: 0.053s, episode steps: 8, steps per second: 150, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.164 [-1.544, 2.545], loss: 1.652088, mean_absolute_error: 4.514420, mean_q: 8.492369\n",
      " 1772/5000: episode: 164, duration: 0.080s, episode steps: 10, steps per second: 125, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.120 [-1.394, 2.118], loss: 2.267915, mean_absolute_error: 4.735408, mean_q: 8.778211\n",
      " 1784/5000: episode: 165, duration: 0.074s, episode steps: 12, steps per second: 162, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.109 [-1.035, 1.743], loss: 1.820438, mean_absolute_error: 4.668440, mean_q: 8.578114\n",
      " 1797/5000: episode: 166, duration: 0.079s, episode steps: 13, steps per second: 165, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.101 [-0.773, 1.281], loss: 3.025274, mean_absolute_error: 4.925613, mean_q: 8.962751\n",
      " 1814/5000: episode: 167, duration: 0.121s, episode steps: 17, steps per second: 141, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.102 [-0.547, 0.971], loss: 1.204149, mean_absolute_error: 4.684546, mean_q: 8.780514\n",
      " 1834/5000: episode: 168, duration: 0.134s, episode steps: 20, steps per second: 150, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.064 [-0.597, 1.020], loss: 1.668318, mean_absolute_error: 4.789892, mean_q: 8.956980\n",
      " 1846/5000: episode: 169, duration: 0.073s, episode steps: 12, steps per second: 165, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.114 [-0.960, 1.579], loss: 2.314873, mean_absolute_error: 4.794576, mean_q: 8.925547\n",
      " 1860/5000: episode: 170, duration: 0.071s, episode steps: 14, steps per second: 197, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.088 [-1.018, 1.579], loss: 1.793485, mean_absolute_error: 4.868046, mean_q: 9.045991\n",
      " 1870/5000: episode: 171, duration: 0.056s, episode steps: 10, steps per second: 179, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.141 [-0.945, 1.660], loss: 0.653452, mean_absolute_error: 4.891116, mean_q: 9.211930\n",
      " 1884/5000: episode: 172, duration: 0.101s, episode steps: 14, steps per second: 139, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.100 [-0.936, 1.574], loss: 2.380238, mean_absolute_error: 5.021603, mean_q: 9.208035\n",
      " 1894/5000: episode: 173, duration: 0.064s, episode steps: 10, steps per second: 157, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.101 [-1.031, 1.594], loss: 2.137447, mean_absolute_error: 5.175389, mean_q: 9.532717\n",
      " 1904/5000: episode: 174, duration: 0.064s, episode steps: 10, steps per second: 156, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.127 [-0.768, 1.504], loss: 0.458606, mean_absolute_error: 4.864738, mean_q: 9.245734\n",
      " 1918/5000: episode: 175, duration: 0.082s, episode steps: 14, steps per second: 171, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.079 [-1.006, 1.640], loss: 2.452979, mean_absolute_error: 5.114629, mean_q: 9.575284\n",
      " 1931/5000: episode: 176, duration: 0.078s, episode steps: 13, steps per second: 166, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.120 [-0.959, 1.475], loss: 2.526259, mean_absolute_error: 4.954045, mean_q: 9.164816\n",
      " 1941/5000: episode: 177, duration: 0.069s, episode steps: 10, steps per second: 145, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.139 [-1.388, 2.277], loss: 3.588377, mean_absolute_error: 5.355469, mean_q: 9.666103\n",
      " 1950/5000: episode: 178, duration: 0.060s, episode steps: 9, steps per second: 150, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.129 [-0.988, 1.700], loss: 2.003539, mean_absolute_error: 5.143177, mean_q: 9.497091\n",
      " 1966/5000: episode: 179, duration: 0.091s, episode steps: 16, steps per second: 175, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.076 [-0.965, 1.537], loss: 2.341374, mean_absolute_error: 5.154332, mean_q: 9.522271\n",
      " 1977/5000: episode: 180, duration: 0.066s, episode steps: 11, steps per second: 165, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.114 [-0.951, 1.509], loss: 3.207646, mean_absolute_error: 5.221879, mean_q: 9.506779\n",
      " 1992/5000: episode: 181, duration: 0.108s, episode steps: 15, steps per second: 138, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.078 [-0.826, 1.421], loss: 2.389696, mean_absolute_error: 5.258954, mean_q: 9.665640\n",
      " 2005/5000: episode: 182, duration: 0.068s, episode steps: 13, steps per second: 192, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.098 [-1.000, 1.647], loss: 2.115984, mean_absolute_error: 5.236845, mean_q: 9.646667\n",
      " 2018/5000: episode: 183, duration: 0.082s, episode steps: 13, steps per second: 159, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.076 [-1.026, 1.465], loss: 3.112484, mean_absolute_error: 5.463452, mean_q: 9.966393\n",
      " 2029/5000: episode: 184, duration: 0.076s, episode steps: 11, steps per second: 145, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.127 [-0.774, 1.413], loss: 2.823155, mean_absolute_error: 5.360818, mean_q: 9.860767\n",
      " 2039/5000: episode: 185, duration: 0.066s, episode steps: 10, steps per second: 151, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.114 [-0.982, 1.573], loss: 2.177821, mean_absolute_error: 5.424601, mean_q: 9.926471\n",
      " 2052/5000: episode: 186, duration: 0.080s, episode steps: 13, steps per second: 163, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.092 [-0.976, 1.653], loss: 2.006085, mean_absolute_error: 5.219761, mean_q: 9.602100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2061/5000: episode: 187, duration: 0.068s, episode steps: 9, steps per second: 131, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.126 [-1.409, 2.134], loss: 2.026814, mean_absolute_error: 5.331370, mean_q: 9.941029\n",
      " 2070/5000: episode: 188, duration: 0.061s, episode steps: 9, steps per second: 147, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.157 [-1.197, 1.989], loss: 1.934086, mean_absolute_error: 5.314934, mean_q: 9.943865\n",
      " 2079/5000: episode: 189, duration: 0.060s, episode steps: 9, steps per second: 151, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.121 [-1.348, 2.193], loss: 2.980314, mean_absolute_error: 5.467113, mean_q: 10.107325\n",
      " 2088/5000: episode: 190, duration: 0.063s, episode steps: 9, steps per second: 143, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.146 [-1.372, 2.276], loss: 3.741415, mean_absolute_error: 5.524062, mean_q: 10.170038\n",
      " 2098/5000: episode: 191, duration: 0.071s, episode steps: 10, steps per second: 141, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.124 [-1.359, 2.132], loss: 2.774938, mean_absolute_error: 5.463942, mean_q: 10.121982\n",
      " 2110/5000: episode: 192, duration: 0.089s, episode steps: 12, steps per second: 135, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.111 [-1.194, 1.931], loss: 4.454000, mean_absolute_error: 5.704370, mean_q: 10.192504\n",
      " 2124/5000: episode: 193, duration: 0.096s, episode steps: 14, steps per second: 146, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.071 [-1.391, 2.081], loss: 2.374333, mean_absolute_error: 5.395056, mean_q: 9.900839\n",
      " 2135/5000: episode: 194, duration: 0.072s, episode steps: 11, steps per second: 152, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.129 [-1.322, 2.168], loss: 2.911411, mean_absolute_error: 5.503135, mean_q: 10.054929\n",
      " 2144/5000: episode: 195, duration: 0.055s, episode steps: 9, steps per second: 165, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.154 [-1.132, 1.968], loss: 3.017207, mean_absolute_error: 5.567854, mean_q: 10.198307\n",
      " 2154/5000: episode: 196, duration: 0.053s, episode steps: 10, steps per second: 189, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.124 [-1.197, 1.841], loss: 2.996338, mean_absolute_error: 5.615273, mean_q: 10.290193\n",
      " 2164/5000: episode: 197, duration: 0.058s, episode steps: 10, steps per second: 172, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.132 [-1.201, 1.830], loss: 2.191190, mean_absolute_error: 5.667631, mean_q: 10.485790\n",
      " 2173/5000: episode: 198, duration: 0.069s, episode steps: 9, steps per second: 130, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.155 [-1.152, 1.970], loss: 2.845542, mean_absolute_error: 5.591314, mean_q: 10.180207\n",
      " 2184/5000: episode: 199, duration: 0.062s, episode steps: 11, steps per second: 178, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.130 [-1.207, 1.933], loss: 3.793169, mean_absolute_error: 5.588325, mean_q: 10.139144\n",
      " 2193/5000: episode: 200, duration: 0.053s, episode steps: 9, steps per second: 170, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.146 [-1.148, 1.926], loss: 1.384270, mean_absolute_error: 5.360670, mean_q: 10.012403\n",
      " 2202/5000: episode: 201, duration: 0.053s, episode steps: 9, steps per second: 168, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.129 [-1.420, 2.210], loss: 3.349803, mean_absolute_error: 5.763578, mean_q: 10.515011\n",
      " 2211/5000: episode: 202, duration: 0.061s, episode steps: 9, steps per second: 149, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.142 [-1.379, 2.260], loss: 3.030643, mean_absolute_error: 5.596913, mean_q: 10.303170\n",
      " 2221/5000: episode: 203, duration: 0.067s, episode steps: 10, steps per second: 148, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.159 [-1.324, 2.184], loss: 2.203113, mean_absolute_error: 5.634692, mean_q: 10.417356\n",
      " 2230/5000: episode: 204, duration: 0.062s, episode steps: 9, steps per second: 145, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.137 [-1.201, 1.964], loss: 2.824966, mean_absolute_error: 5.631402, mean_q: 10.270752\n",
      " 2242/5000: episode: 205, duration: 0.084s, episode steps: 12, steps per second: 143, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.125 [-1.132, 1.984], loss: 4.081137, mean_absolute_error: 5.791930, mean_q: 10.366006\n",
      " 2253/5000: episode: 206, duration: 0.063s, episode steps: 11, steps per second: 175, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.108 [-1.196, 1.893], loss: 2.921077, mean_absolute_error: 5.617470, mean_q: 10.194261\n",
      " 2264/5000: episode: 207, duration: 0.080s, episode steps: 11, steps per second: 137, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.122 [-1.178, 1.804], loss: 2.320124, mean_absolute_error: 5.798853, mean_q: 10.602371\n",
      " 2275/5000: episode: 208, duration: 0.086s, episode steps: 11, steps per second: 128, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.129 [-1.131, 1.859], loss: 2.866620, mean_absolute_error: 5.672167, mean_q: 10.385624\n",
      " 2283/5000: episode: 209, duration: 0.054s, episode steps: 8, steps per second: 148, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.128 [-1.418, 2.177], loss: 2.489563, mean_absolute_error: 5.856270, mean_q: 10.764848\n",
      " 2293/5000: episode: 210, duration: 0.073s, episode steps: 10, steps per second: 137, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.127 [-1.406, 2.155], loss: 3.519992, mean_absolute_error: 5.602562, mean_q: 10.092253\n",
      " 2303/5000: episode: 211, duration: 0.057s, episode steps: 10, steps per second: 177, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.115 [-1.222, 2.042], loss: 1.464109, mean_absolute_error: 5.621017, mean_q: 10.440208\n",
      " 2317/5000: episode: 212, duration: 0.098s, episode steps: 14, steps per second: 143, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.114 [-1.146, 1.975], loss: 3.639932, mean_absolute_error: 5.817657, mean_q: 10.544969\n",
      " 2327/5000: episode: 213, duration: 0.081s, episode steps: 10, steps per second: 124, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.134 [-1.166, 1.997], loss: 2.242143, mean_absolute_error: 5.583908, mean_q: 10.261854\n",
      " 2337/5000: episode: 214, duration: 0.073s, episode steps: 10, steps per second: 137, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.125 [-1.143, 1.826], loss: 4.614251, mean_absolute_error: 5.986238, mean_q: 10.602115\n",
      " 2347/5000: episode: 215, duration: 0.068s, episode steps: 10, steps per second: 148, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.135 [-0.981, 1.614], loss: 1.739474, mean_absolute_error: 5.516826, mean_q: 10.111818\n",
      " 2360/5000: episode: 216, duration: 0.090s, episode steps: 13, steps per second: 144, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.099 [-0.997, 1.495], loss: 2.679086, mean_absolute_error: 5.595160, mean_q: 10.214558\n",
      " 2372/5000: episode: 217, duration: 0.086s, episode steps: 12, steps per second: 140, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.105 [-1.215, 1.722], loss: 2.085107, mean_absolute_error: 5.502033, mean_q: 10.225975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2383/5000: episode: 218, duration: 0.075s, episode steps: 11, steps per second: 147, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.108 [-1.158, 1.829], loss: 3.150957, mean_absolute_error: 5.749426, mean_q: 10.501872\n",
      " 2394/5000: episode: 219, duration: 0.064s, episode steps: 11, steps per second: 171, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.109 [-0.974, 1.561], loss: 3.416447, mean_absolute_error: 5.719017, mean_q: 10.337673\n",
      " 2405/5000: episode: 220, duration: 0.085s, episode steps: 11, steps per second: 130, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.110 [-1.019, 1.581], loss: 2.972915, mean_absolute_error: 5.771017, mean_q: 10.521905\n",
      " 2417/5000: episode: 221, duration: 0.081s, episode steps: 12, steps per second: 147, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.097 [-0.798, 1.277], loss: 2.627394, mean_absolute_error: 5.701572, mean_q: 10.450202\n",
      " 2427/5000: episode: 222, duration: 0.065s, episode steps: 10, steps per second: 154, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.139 [-1.132, 1.850], loss: 4.446278, mean_absolute_error: 5.901056, mean_q: 10.490531\n",
      " 2437/5000: episode: 223, duration: 0.071s, episode steps: 10, steps per second: 140, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.105 [-1.194, 1.792], loss: 2.179831, mean_absolute_error: 5.714530, mean_q: 10.498106\n",
      " 2451/5000: episode: 224, duration: 0.094s, episode steps: 14, steps per second: 148, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.101 [-0.927, 1.462], loss: 3.377223, mean_absolute_error: 5.805977, mean_q: 10.572065\n",
      " 2461/5000: episode: 225, duration: 0.066s, episode steps: 10, steps per second: 151, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.138 [-1.178, 1.952], loss: 2.301571, mean_absolute_error: 5.692774, mean_q: 10.612238\n",
      " 2472/5000: episode: 226, duration: 0.080s, episode steps: 11, steps per second: 138, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.111 [-1.340, 2.122], loss: 3.412788, mean_absolute_error: 5.862314, mean_q: 10.652284\n",
      " 2482/5000: episode: 227, duration: 0.075s, episode steps: 10, steps per second: 133, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.127 [-1.383, 2.132], loss: 2.864010, mean_absolute_error: 5.819516, mean_q: 10.720703\n",
      " 2496/5000: episode: 228, duration: 0.100s, episode steps: 14, steps per second: 141, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.107 [-1.349, 2.159], loss: 3.279396, mean_absolute_error: 5.799868, mean_q: 10.565984\n",
      " 2507/5000: episode: 229, duration: 0.075s, episode steps: 11, steps per second: 148, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.102 [-1.406, 2.088], loss: 1.871833, mean_absolute_error: 5.761732, mean_q: 10.705175\n",
      " 2518/5000: episode: 230, duration: 0.078s, episode steps: 11, steps per second: 141, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.098 [-1.415, 2.031], loss: 3.438873, mean_absolute_error: 5.824968, mean_q: 10.627201\n",
      " 2528/5000: episode: 231, duration: 0.077s, episode steps: 10, steps per second: 130, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.162 [-1.344, 2.292], loss: 2.399252, mean_absolute_error: 5.782887, mean_q: 10.607501\n",
      " 2540/5000: episode: 232, duration: 0.071s, episode steps: 12, steps per second: 169, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.087 [-1.213, 1.770], loss: 2.571019, mean_absolute_error: 5.754869, mean_q: 10.573609\n",
      " 2549/5000: episode: 233, duration: 0.060s, episode steps: 9, steps per second: 150, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.140 [-1.182, 1.879], loss: 2.780541, mean_absolute_error: 5.893595, mean_q: 10.764967\n",
      " 2561/5000: episode: 234, duration: 0.077s, episode steps: 12, steps per second: 156, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.120 [-1.160, 1.952], loss: 2.279060, mean_absolute_error: 5.718431, mean_q: 10.561624\n",
      " 2570/5000: episode: 235, duration: 0.057s, episode steps: 9, steps per second: 159, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.128 [-1.225, 1.918], loss: 2.821269, mean_absolute_error: 5.845791, mean_q: 10.770488\n",
      " 2579/5000: episode: 236, duration: 0.064s, episode steps: 9, steps per second: 140, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.146 [-1.150, 1.926], loss: 2.352169, mean_absolute_error: 5.688358, mean_q: 10.466073\n",
      " 2588/5000: episode: 237, duration: 0.055s, episode steps: 9, steps per second: 163, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.116 [-1.026, 1.641], loss: 1.779310, mean_absolute_error: 5.558804, mean_q: 10.276337\n",
      " 2601/5000: episode: 238, duration: 0.087s, episode steps: 13, steps per second: 150, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.092 [-0.819, 1.312], loss: 2.673816, mean_absolute_error: 5.770694, mean_q: 10.607285\n",
      " 2613/5000: episode: 239, duration: 0.087s, episode steps: 12, steps per second: 137, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.097 [-1.166, 1.719], loss: 3.454723, mean_absolute_error: 5.728844, mean_q: 10.404842\n",
      " 2626/5000: episode: 240, duration: 0.082s, episode steps: 13, steps per second: 158, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.116 [-0.985, 1.541], loss: 2.578778, mean_absolute_error: 5.800712, mean_q: 10.636012\n",
      " 2641/5000: episode: 241, duration: 0.107s, episode steps: 15, steps per second: 140, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.059 [-1.220, 1.783], loss: 1.978804, mean_absolute_error: 5.635165, mean_q: 10.466638\n",
      " 2652/5000: episode: 242, duration: 0.096s, episode steps: 11, steps per second: 115, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.143 [-1.132, 1.830], loss: 2.625782, mean_absolute_error: 5.750068, mean_q: 10.594378\n",
      " 2663/5000: episode: 243, duration: 0.080s, episode steps: 11, steps per second: 138, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.089 [-1.019, 1.598], loss: 2.415048, mean_absolute_error: 5.788141, mean_q: 10.750295\n",
      " 2673/5000: episode: 244, duration: 0.064s, episode steps: 10, steps per second: 157, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.109 [-1.029, 1.544], loss: 3.061794, mean_absolute_error: 5.922352, mean_q: 10.902620\n",
      " 2683/5000: episode: 245, duration: 0.060s, episode steps: 10, steps per second: 168, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.110 [-1.018, 1.529], loss: 2.828516, mean_absolute_error: 5.856114, mean_q: 10.709712\n",
      " 2695/5000: episode: 246, duration: 0.084s, episode steps: 12, steps per second: 142, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.099 [-0.954, 1.484], loss: 3.072715, mean_absolute_error: 5.719656, mean_q: 10.439675\n",
      " 2705/5000: episode: 247, duration: 0.068s, episode steps: 10, steps per second: 147, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.108 [-0.829, 1.353], loss: 2.062115, mean_absolute_error: 5.812617, mean_q: 10.763744\n",
      " 2722/5000: episode: 248, duration: 0.105s, episode steps: 17, steps per second: 161, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.062 [-1.012, 1.406], loss: 2.140444, mean_absolute_error: 5.799426, mean_q: 10.759953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2738/5000: episode: 249, duration: 0.101s, episode steps: 16, steps per second: 158, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.094 [-0.954, 1.440], loss: 1.949209, mean_absolute_error: 5.700992, mean_q: 10.550403\n",
      " 2749/5000: episode: 250, duration: 0.064s, episode steps: 11, steps per second: 171, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.128 [-0.772, 1.248], loss: 2.351872, mean_absolute_error: 5.773325, mean_q: 10.736169\n",
      " 2775/5000: episode: 251, duration: 0.147s, episode steps: 26, steps per second: 177, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.076 [-0.592, 0.885], loss: 2.110947, mean_absolute_error: 5.827948, mean_q: 10.873758\n",
      " 2798/5000: episode: 252, duration: 0.139s, episode steps: 23, steps per second: 165, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.144 [-0.541, 0.961], loss: 2.705421, mean_absolute_error: 5.797556, mean_q: 10.664344\n",
      " 2824/5000: episode: 253, duration: 0.165s, episode steps: 26, steps per second: 157, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.108 [-0.221, 0.836], loss: 2.801640, mean_absolute_error: 5.853108, mean_q: 10.688609\n",
      " 2849/5000: episode: 254, duration: 0.162s, episode steps: 25, steps per second: 154, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.100 [-0.552, 0.891], loss: 2.175243, mean_absolute_error: 5.713422, mean_q: 10.566551\n",
      " 2875/5000: episode: 255, duration: 0.148s, episode steps: 26, steps per second: 176, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.075 [-0.367, 0.785], loss: 2.352991, mean_absolute_error: 5.798339, mean_q: 10.681808\n",
      " 2895/5000: episode: 256, duration: 0.139s, episode steps: 20, steps per second: 144, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.117 [-0.552, 0.984], loss: 2.218942, mean_absolute_error: 5.791091, mean_q: 10.739813\n",
      " 2927/5000: episode: 257, duration: 0.218s, episode steps: 32, steps per second: 147, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.115 [-0.374, 0.924], loss: 2.220917, mean_absolute_error: 5.757940, mean_q: 10.661238\n",
      " 2968/5000: episode: 258, duration: 0.285s, episode steps: 41, steps per second: 144, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: 0.096 [-0.400, 0.687], loss: 1.880092, mean_absolute_error: 5.754844, mean_q: 10.760117\n",
      " 3020/5000: episode: 259, duration: 0.350s, episode steps: 52, steps per second: 149, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.119 [-0.986, 0.386], loss: 2.579993, mean_absolute_error: 5.851437, mean_q: 10.828218\n",
      " 3069/5000: episode: 260, duration: 0.374s, episode steps: 49, steps per second: 131, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.140 [-0.541, 1.101], loss: 2.151110, mean_absolute_error: 5.826462, mean_q: 10.810836\n",
      " 3112/5000: episode: 261, duration: 0.253s, episode steps: 43, steps per second: 170, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.084 [-0.306, 0.858], loss: 2.357802, mean_absolute_error: 5.896586, mean_q: 10.924975\n",
      " 3139/5000: episode: 262, duration: 0.182s, episode steps: 27, steps per second: 148, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.100 [-0.212, 0.667], loss: 1.965476, mean_absolute_error: 5.950443, mean_q: 11.120430\n",
      " 3190/5000: episode: 263, duration: 0.348s, episode steps: 51, steps per second: 147, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.097 [-0.603, 0.803], loss: 2.466062, mean_absolute_error: 5.974878, mean_q: 11.046171\n",
      " 3233/5000: episode: 264, duration: 0.309s, episode steps: 43, steps per second: 139, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.122 [-0.348, 0.666], loss: 1.927438, mean_absolute_error: 5.942783, mean_q: 11.104391\n",
      " 3277/5000: episode: 265, duration: 0.272s, episode steps: 44, steps per second: 162, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.091 [-0.772, 0.425], loss: 1.898044, mean_absolute_error: 5.982787, mean_q: 11.210633\n",
      " 3299/5000: episode: 266, duration: 0.139s, episode steps: 22, steps per second: 158, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.129 [-0.704, 0.378], loss: 2.276094, mean_absolute_error: 5.925443, mean_q: 11.089391\n",
      " 3390/5000: episode: 267, duration: 0.567s, episode steps: 91, steps per second: 160, episode reward: 91.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.095 [-0.780, 0.996], loss: 2.060775, mean_absolute_error: 6.116637, mean_q: 11.440770\n",
      " 3473/5000: episode: 268, duration: 0.571s, episode steps: 83, steps per second: 145, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.094 [-0.461, 0.958], loss: 2.258102, mean_absolute_error: 6.258460, mean_q: 11.670248\n",
      " 3536/5000: episode: 269, duration: 0.412s, episode steps: 63, steps per second: 153, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.042 [-0.812, 0.583], loss: 2.138644, mean_absolute_error: 6.254130, mean_q: 11.704853\n",
      " 3580/5000: episode: 270, duration: 0.295s, episode steps: 44, steps per second: 149, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.139 [-0.732, 0.386], loss: 2.513430, mean_absolute_error: 6.316610, mean_q: 11.700507\n",
      " 3672/5000: episode: 271, duration: 0.584s, episode steps: 92, steps per second: 157, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.102 [-0.528, 0.955], loss: 2.347378, mean_absolute_error: 6.429597, mean_q: 11.990876\n",
      " 3822/5000: episode: 272, duration: 1.030s, episode steps: 150, steps per second: 146, episode reward: 150.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.052 [-0.590, 0.903], loss: 2.409675, mean_absolute_error: 6.493885, mean_q: 12.081133\n",
      " 3869/5000: episode: 273, duration: 0.324s, episode steps: 47, steps per second: 145, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.468 [0.000, 1.000], mean observation: -0.119 [-0.797, 0.415], loss: 2.142900, mean_absolute_error: 6.552296, mean_q: 12.252998\n",
      " 3933/5000: episode: 274, duration: 0.411s, episode steps: 64, steps per second: 156, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.040 [-0.566, 1.105], loss: 2.337878, mean_absolute_error: 6.640612, mean_q: 12.449965\n",
      " 3958/5000: episode: 275, duration: 0.180s, episode steps: 25, steps per second: 139, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.560 [0.000, 1.000], mean observation: 0.098 [-0.433, 0.755], loss: 2.612428, mean_absolute_error: 6.551494, mean_q: 12.183903\n",
      " 4038/5000: episode: 276, duration: 0.509s, episode steps: 80, steps per second: 157, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.127 [-0.498, 1.063], loss: 1.935456, mean_absolute_error: 6.695680, mean_q: 12.561666\n",
      " 4098/5000: episode: 277, duration: 0.383s, episode steps: 60, steps per second: 157, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.206 [-0.351, 1.134], loss: 2.352060, mean_absolute_error: 6.846574, mean_q: 12.818355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4162/5000: episode: 278, duration: 0.424s, episode steps: 64, steps per second: 151, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.095 [-0.763, 0.347], loss: 1.994943, mean_absolute_error: 6.932982, mean_q: 13.010740\n",
      " 4284/5000: episode: 279, duration: 0.832s, episode steps: 122, steps per second: 147, episode reward: 122.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.061 [-0.733, 0.594], loss: 2.477951, mean_absolute_error: 7.008595, mean_q: 13.104886\n",
      " 4318/5000: episode: 280, duration: 0.240s, episode steps: 34, steps per second: 141, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.441 [0.000, 1.000], mean observation: -0.133 [-0.756, 0.444], loss: 1.990718, mean_absolute_error: 7.177884, mean_q: 13.540406\n",
      " 4389/5000: episode: 281, duration: 0.474s, episode steps: 71, steps per second: 150, episode reward: 71.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.127 [-1.019, 0.496], loss: 2.304615, mean_absolute_error: 7.258020, mean_q: 13.642248\n",
      " 4485/5000: episode: 282, duration: 0.583s, episode steps: 96, steps per second: 165, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.019 [-0.774, 0.378], loss: 2.202954, mean_absolute_error: 7.320709, mean_q: 13.814926\n",
      " 4550/5000: episode: 283, duration: 0.401s, episode steps: 65, steps per second: 162, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.143 [-0.915, 0.384], loss: 2.185049, mean_absolute_error: 7.380893, mean_q: 13.886477\n",
      " 4611/5000: episode: 284, duration: 0.410s, episode steps: 61, steps per second: 149, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.095 [-1.224, 0.535], loss: 2.390679, mean_absolute_error: 7.449384, mean_q: 14.015282\n",
      " 4696/5000: episode: 285, duration: 0.568s, episode steps: 85, steps per second: 150, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.110 [-0.615, 1.050], loss: 2.509807, mean_absolute_error: 7.550984, mean_q: 14.200853\n",
      " 4771/5000: episode: 286, duration: 0.482s, episode steps: 75, steps per second: 156, episode reward: 75.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.072 [-0.273, 0.695], loss: 2.127429, mean_absolute_error: 7.695101, mean_q: 14.563848\n",
      " 4825/5000: episode: 287, duration: 0.358s, episode steps: 54, steps per second: 151, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.082 [-0.233, 0.916], loss: 2.609607, mean_absolute_error: 7.860145, mean_q: 14.845247\n",
      " 4853/5000: episode: 288, duration: 0.191s, episode steps: 28, steps per second: 146, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.121 [-0.364, 0.628], loss: 2.044798, mean_absolute_error: 7.792243, mean_q: 14.795194\n",
      " 4916/5000: episode: 289, duration: 0.421s, episode steps: 63, steps per second: 150, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: 0.214 [-0.414, 1.629], loss: 2.992583, mean_absolute_error: 7.953261, mean_q: 14.939770\n",
      " 4954/5000: episode: 290, duration: 0.248s, episode steps: 38, steps per second: 153, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.088 [-0.815, 0.443], loss: 2.883645, mean_absolute_error: 8.007107, mean_q: 15.014854\n",
      " 4995/5000: episode: 291, duration: 0.278s, episode steps: 41, steps per second: 148, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.171 [-0.867, 0.182], loss: 2.877816, mean_absolute_error: 7.946102, mean_q: 14.889176\n",
      "done, took 33.701 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b17e646c18>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#setting agent\n",
    "\n",
    "policy = EpsGreedyQPolicy()\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory = memory, nb_steps_warmup = 10, target_model_update=1e-2, policy =policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=5000, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
